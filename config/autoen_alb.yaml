dataset_params_shading:
  im_path: '/home/project/dataset/Hyperism/ldr_sh_fk'
  im_channels : 1
  im_size : 256
  name: 'ldr_to_sh'

dataset_params:
  im_path: '/home/project/dataset/Hyperism/hyperism_ldr_albedo_ground_truth_256'
  im_channels : 3
  im_size : 256
  name: 'ldr_to_sh'

dataset_params_input:
  im_path: '/home/project/dataset/Hyperism/hyperism_ldr_dequantize_linearize_256'
  im_channels :  3
  im_size : 256
  name: 'ldr_to_sh'

autoencoder_params:
  z_channels: 256
  down_channels : [32, 64, 128, 256, 256]
  mid_channels : [256, 256, 256, 256]
  down_sample : [True, True, True, False]
  attn_down : [True, True, True, True]
  norm_channels: 32
  num_heads: 16
  num_down_layers : 4
  num_mid_layers : 4
  num_up_layers : 4

ldm_params:
  down_channels: [ 256, 512]
  mid_channels: [512, 256, 256]
  down_sample: [False]
  attn_down : [True]
  time_emb_dim: 256
  norm_channels : 128
  num_heads : 16
  conv_out_channels : 256
  num_down_layers: 2
  num_mid_layers: 2
  num_up_layers: 2

encoder_params:
  down_channels:  [ 64, 64, 128, 128, 256, 256, 512]  # Starting with 32 channels, ending with 256
  mid_channels: [512, 256, 256]           # Mid channels should start with last down_channel
  down_sample: [True, True, True,True, True, False]    # Downsampling on first 3 transitions
  num_down_layers: 2               # Number of layers in each DownBlock
  num_mid_layers: 2                # Number of layers in each MidBlock
  attn_down: [False,False,False,False,False,False]     # Attention in second and third DownBlocks
  norm_channels: 32                 # Normalization channels
  num_heads: 16                       # Number of attention heads 

train_params:
  im_size_lt : 8
  seed : 107
  task_name: 'ldr_to_sh'
  ldm_batch_size: 32
  autoencoder_batch_size: 64
  disc_start: 21000
  disc_weight: 0.1
  perceptual_weight: 1
  kl_weight: 0.005
  gradient_weight: 0
  ldm_epochs : 1000
  autoencoder_epochs : 2000
  num_samples : 25
  ldm_lr: 0.00001
  albedo_weight : 1
  shading_weight : 0
  autoencoder_lr: 0.0001
  discriminator_lr: 0.00005
  autoencoder_acc_steps : 1
  autoencoder_img_save_steps : 1050
  save_latents : False
  vae_latent_dir_name : 'latent_output\latent_vectors.h5'
  ldm_ckpt_name: 'flow_model_ckpt.pth'
  vae_autoencoder_ckpt_name: 'epoch_41_best_autoencoder_model_checkpoint.pth'
  vae_discriminator_ckpt_name: 'vae_discriminator_ckpt.pth'  
  encoder_ckpt_name: 'encoder_ckpt.pth'  
  ldm_optimizer_ckpt_name: 'ldm_optimizer_ckpt.pth'  